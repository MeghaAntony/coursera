{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from datetime import datetime\n",
    "import elasticsearch\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "COLORS = [\"slategrey\",\"darkorange\",\"royalblue\"]\n",
    "\n",
    "def fetch_data(data_dir):\n",
    "\n",
    "   \n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise Exception(\"specified data dir does not exist\")\n",
    "    if not len(os.listdir(data_dir)) > 0:\n",
    "        raise Exception(\"specified data dir does not contain any files\")\n",
    "\n",
    "    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search(\"\\.json\",f)]\n",
    "    correct_columns = [\"country\", \"customer_id\", \"day\", \"invoice\", \"month\",\n",
    "                       \"price\", \"stream_id\", \"times_viewed\", \"year\"]\n",
    "\n",
    "    ##############################\n",
    "    all_months = {}\n",
    "    for file_name in file_list:\n",
    "        data = pd.read_json(file_name)\n",
    "        all_months[os.path.split(file_name)[-1]] = data\n",
    "\n",
    "    #format data with correct column names \n",
    "    for f,data in all_months.items():\n",
    "        cols = set(data.columns.tolist())\n",
    "        if 'total_price' in cols:\n",
    "            data.rename(columns={'total_price':'price'},inplace=True)\n",
    "        if 'TimesViewed' in cols:\n",
    "            data.rename(columns={'TimesViewed':'times_viewed'},inplace=True)\n",
    "        if 'StreamID' in cols:\n",
    "             data.rename(columns={'StreamID':'stream_id'},inplace=True)\n",
    "        \n",
    "\n",
    "        cols = data.columns.tolist()\n",
    "        if sorted(cols) != correct_columns:\n",
    "            raise Exception(\"columns name could not be matched to correct cols\")\n",
    "\n",
    "    #data concatenation \n",
    "    data = pd.concat(list(all_months.values()),sort=True)\n",
    "    years,months,days = data['year'].values,data['month'].values,data['day'].values \n",
    "    dates = [\"{}-{}-{}\".format(years[i],str(months[i]).zfill(2),str(days[i]).zfill(2)) for i in range(data.shape[0])]\n",
    "    data['invoice_date'] = np.array(dates,dtype='datetime64[D]')\n",
    "    data['invoice'] = [re.sub(\"\\D+\",\"\",i) for i in data['invoice'].values]\n",
    "    \n",
    "    ## sort by date and reset the index\n",
    "    data.sort_values(by='invoice_date',inplace=True)\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "\n",
    "def convert_to_ts(data_orig, country=None):\n",
    "   \n",
    "\n",
    "    if country:\n",
    "        if country not in np.unique(df_orig['country'].values):\n",
    "            raise Excpetion(\"country not found\")\n",
    "    \n",
    "        mask = data_orig['country'] == country\n",
    "        data = data_orig[mask]\n",
    "    else:\n",
    "        data = data_orig\n",
    "        \n",
    "    ## use a date range to ensure all days are accounted for in the data\n",
    "    invoice_dates = df['invoice_date'].values\n",
    "    start_month = '{}-{}'.format(df['year'].values[0],str(data['month'].values[0]).zfill(2))\n",
    "    stop_month = '{}-{}'.format(df['year'].values[-1],str(data['month'].values[-1]).zfill(2))\n",
    "    data_dates = data['invoice_date'].values.astype('datetime64[D]')\n",
    "    days = np.arange(start_month,stop_month,dtype='datetime64[D]')\n",
    "    \n",
    "    purchases = np.array([np.where(df_dates==day)[0].size for day in days])\n",
    "    invoices = [np.unique(data[data_dates==day]['invoice'].values).size for day in days]\n",
    "    streams = [np.unique(data[data_dates==day]['stream_id'].values).size for day in days]\n",
    "    views =  [data[data_dates==day]['times_viewed'].values.sum() for day in days]\n",
    "    revenue = [data[data_dates==day]['price'].values.sum() for day in days]\n",
    "    year_month = [\"-\".join(re.split(\"-\",str(day))[:2]) for day in days]\n",
    "\n",
    "    data_time = pd.DataFrame({'date':days,\n",
    "                            'purchases':purchases,\n",
    "                            'unique_invoices':invoices,\n",
    "                            'unique_streams':streams,\n",
    "                            'total_views':views,\n",
    "                            'year_month':year_month,\n",
    "                            'revenue':revenue})\n",
    "    return(data_time)\n",
    "\n",
    "\n",
    "def fetch_ts(data_dir, clean=False):\n",
    "   \n",
    "    #convenience function to read in new data\n",
    "    \n",
    "  \n",
    "\n",
    "    ts_data_dir = os.path.join(data_dir,\"ts-data\")\n",
    "    \n",
    "    if clean:\n",
    "        shutil.rmtree(ts_data_dir)\n",
    "    if not os.path.exists(ts_data_dir):\n",
    "        os.mkdir(ts_data_dir)\n",
    "\n",
    "         \n",
    "    if len(os.listdir(ts_data_dir)) > 0:\n",
    "        print(\"... loading ts data from files\")\n",
    "        return({re.sub(\"\\.csv\",\"\",cf)[3:]:pd.read_csv(os.path.join(ts_data_dir,cf)) for cf in os.listdir(ts_data_dir)})\n",
    "\n",
    "    ## get original data\n",
    "    print(\"..... processing data\")\n",
    "    df = fetch_data(data_dir)\n",
    "\n",
    "    ## find the top ten countries (wrt revenue)\n",
    "    table = pd.pivot_table(df,index='country',values=\"price\",aggfunc='sum')\n",
    "    table.columns = ['total_revenue']\n",
    "    table.sort_values(by='total_revenue',inplace=True,ascending=False)\n",
    "    top_ten_countries =  np.array(list(table.index))[:10]\n",
    "\n",
    "    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search(\"\\.json\",f)]\n",
    "    countries = [os.path.join(data_dir,\"ts-\"+re.sub(\"\\s+\",\"_\",c.lower()) + \".csv\") for c in top_ten_countries]\n",
    "\n",
    "    ## load the data\n",
    "    dfs = {}\n",
    "    dfs['all'] = convert_to_ts(df)\n",
    "    for country in top_ten_countries:\n",
    "        country_id = re.sub(\"\\s+\",\"_\",country.lower())\n",
    "        file_name = os.path.join(data_dir,\"ts-\"+ country_id + \".csv\")\n",
    "        dfs[country_id] = convert_to_ts(df,country=country)\n",
    "\n",
    "    ## save the data as csvs    \n",
    "    for key, item in dfs.items():\n",
    "        item.to_csv(os.path.join(ts_data_dir,\"ts-\"+key+\".csv\"),index=False)\n",
    "        \n",
    "    return(dfs)\n",
    "\n",
    "def engineer_features(df,training=True):\n",
    "    \"\"\"\n",
    "    for any given day the target becomes the sum of the next days revenue\n",
    "    for that day we engineer several features that help predict the summed revenue\n",
    "    \n",
    "    the 'training' flag will trim data that should not be used for training\n",
    "    when set to false all data will be returned\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## extract dates\n",
    "    dates = df['date'].values.copy()\n",
    "    dates = dates.astype('datetime64[D]')\n",
    "\n",
    "    ## engineer some features\n",
    "    eng_features = defaultdict(list)\n",
    "    previous =[7, 70]  #[7, 14, 21, 28, 35, 42, 49, 56, 63, 70]\n",
    "    y = np.zeros(dates.size)\n",
    "    for d,day in enumerate(dates):\n",
    "\n",
    "        ## use windows in time back from a specific date\n",
    "        for num in previous:\n",
    "            current = np.datetime64(day, 'D') \n",
    "            prev = current - np.timedelta64(num, 'D')\n",
    "            mask = np.in1d(dates, np.arange(prev,current,dtype='datetime64[D]'))\n",
    "            eng_features[\"previous_{}\".format(num)].append(df[mask]['revenue'].sum())\n",
    "\n",
    "        ## get get the target revenue    \n",
    "        plus_30 = current + np.timedelta64(30,'D')\n",
    "        mask = np.in1d(dates, np.arange(current,plus_30,dtype='datetime64[D]'))\n",
    "        y[d] = df[mask]['revenue'].sum()\n",
    "\n",
    "        ## attempt to capture monthly trend with previous years data (if present)\n",
    "        start_date = current - np.timedelta64(365,'D')\n",
    "        stop_date = plus_30 - np.timedelta64(365,'D')\n",
    "        mask = np.in1d(dates, np.arange(start_date,stop_date,dtype='datetime64[D]'))\n",
    "        eng_features['previous_year'].append(df[mask]['revenue'].sum())\n",
    "\n",
    "        ## add some non-revenue features\n",
    "        minus_30 = current - np.timedelta64(30,'D')\n",
    "        mask = np.in1d(dates, np.arange(minus_30,current,dtype='datetime64[D]'))\n",
    "        eng_features['recent_invoices'].append(df[mask]['unique_invoices'].mean())\n",
    "        eng_features['recent_views'].append(df[mask]['total_views'].mean())\n",
    "\n",
    "    X = pd.DataFrame(eng_features)\n",
    "    ## combine features in to df and remove rows with all zeros\n",
    "    X.fillna(0,inplace=True)\n",
    "    mask = X.sum(axis=1)>0\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    dates = dates[mask]\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if training == True:\n",
    "        ## remove the last 30 days (because the target is not reliable)\n",
    "        mask = np.arange(X.shape[0]) < np.arange(X.shape[0])[-30]\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        dates = dates[mask]\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "\n",
    "\n",
    "#    print(X)\n",
    "\n",
    "#    dfplot1 = X[['previous_7']].copy()\n",
    "#    dfplot2 = X[['previous_70']].copy()\n",
    "#    dfplot3 = X[['previous_year']].copy()\n",
    "    \n",
    "#    fig = plt.figure(figsize=(10.0, 8.0))\n",
    "#    ax = fig.add_subplot(111)\n",
    "#    plt.plot(dfplot1, \"r\", label=\"pre7days\")\n",
    "#    plt.plot(dfplot2, \"g\", label=\"pre70days\")\n",
    "#    plt.plot(dfplot3, \"b\", label=\"pre1year\")\n",
    "#    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=18)\n",
    "#    plt.show()\n",
    "\n",
    "    return(X,y,dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from application.cslib import fetch_ts, engineer_features\n",
    "\n",
    "\n",
    "\n",
    "data_dir = os.path.join(\"data\",\"cs-train\")\n",
    "\n",
    "ts_all = fetch_ts(data_dir,clean=False)\n",
    "\n",
    "X,y,dates = engineer_features(ts_all['all'])\n",
    "        \n",
    "## Perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)\n",
    "pipe_lr = Pipeline([('scaling', StandardScaler()),('regr',LinearRegression())])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "                           \n",
    "y_pred_base = pipe_lr.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'rf__criterion': ['mse','mae'],\n",
    "    'rf__n_estimators': [10,15,20,25,50,100]\n",
    "    }\n",
    "\n",
    "pipe_rf = Pipeline(steps=[('scaler', StandardScaler()), ('rf', RandomForestRegressor())])\n",
    "\n",
    "grid = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(dates, y,label=\"groundtruth\") \n",
    "plt.plot(dates, y_pred_base,label=\"baseline\") \n",
    "plt.plot(dates, y_pred,label=\"model\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_start = time.time() \n",
    "    data_dir = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\")\n",
    "    \n",
    "    print(\"...reading data to store pd dataframe\")    \n",
    "    df_time = convert_to_ts(fetch_data(data_dir), country=None)\n",
    "    print(df_time)\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"\")\n",
    "    print(\"load time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "\n",
    "...reading data to store pd dataframe\n",
    "          date  purchases  unique_invoices  unique_streams  total_views  \\\n",
    "0   2017-11-01          0                0               0            0   \n",
    "1   2017-11-02          0                0               0            0   \n",
    "2   2017-11-03          0                0               0            0   \n",
    "3   2017-11-04          0                0               0            0   \n",
    "4   2017-11-05          0                0               0            0   \n",
    "..         ...        ...              ...             ...          ...   \n",
    "602 2019-06-26       1358               67             999         6420   \n",
    "603 2019-06-27       1620               80             944         9435   \n",
    "604 2019-06-28       1027               70             607         5539   \n",
    "605 2019-06-29          0                0               0            0   \n",
    "606 2019-06-30        602               27             423         2534   \n",
    "\n",
    "    year_month  revenue  \n",
    "0      2017-11     0.00  \n",
    "1      2017-11     0.00  \n",
    "2      2017-11     0.00  \n",
    "3      2017-11     0.00  \n",
    "4      2017-11     0.00  \n",
    "..         ...      ...  \n",
    "602    2019-06  4903.17  \n",
    "603    2019-06  5499.38  \n",
    "604    2019-06  3570.60  \n",
    "605    2019-06     0.00  \n",
    "606    2019-06  1793.98  \n",
    "\n",
    "[607 rows x 7 columns]\n",
    "\n",
    "load time: 0:00:16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_start = time.time() \n",
    "    data_dir = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\")\n",
    "    \n",
    "    print(\"...fetching data to make csv\")\n",
    "    ts_all = fetch_ts(data_dir, clean=True)\n",
    "    for key,item in ts_all.items():\n",
    "        print(key,item.shape)\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"\")\n",
    "    print(\"load time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "\n",
    "...fetching data to make csv\n",
    "..... processing data\n",
    "all (607, 7)\n",
    "united_kingdom (607, 7)\n",
    "eire (607, 7)\n",
    "germany (607, 7)\n",
    "france (607, 7)\n",
    "norway (577, 7)\n",
    "spain (607, 7)\n",
    "hong_kong (426, 7)\n",
    "portugal (607, 7)\n",
    "singapore (456, 7)\n",
    "netherlands (607, 7)\n",
    "\n",
    "load time: 0:00:51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_start = time.time() \n",
    "    data_dir = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\")\n",
    "    \n",
    "    print(\"...making engineer features\")\n",
    "    engineer_features(df_time,training=True)\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"load time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "\n",
    "...making engineer features\n",
    "load time: 0:00:02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,os,re,csv,sys,uuid,joblib\n",
    "from datetime import date\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# (from logger import update_predict_log, update_train_log)\n",
    "# (from cslib import fetch_ts, engineer_features)\n",
    "\n",
    "## model specific variables (iterate the version and note with each change)\n",
    "MODEL_DIR = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\",\"models\")\n",
    "MODEL_VERSION = 0.1\n",
    "MODEL_VERSION_NOTE = \"supervised learing model for time-series\"\n",
    "\n",
    "def _model_train(df,tag,test=False):\n",
    "    \"\"\"\n",
    "    example funtion to train model\n",
    "    \n",
    "    The 'test' flag when set to 'True':\n",
    "        (1) subsets the data and serializes a test version\n",
    "        (2) specifies that the use of the 'test' log file \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## start timer for runtime\n",
    "    time_start = time.time()\n",
    "    \n",
    "    X,y,dates = engineer_features(df)\n",
    "\n",
    "    if test:\n",
    "        n_samples = int(np.round(0.3 * X.shape[0]))\n",
    "        subset_indices = np.random.choice(np.arange(X.shape[0]),n_samples,\n",
    "                                          replace=False).astype(int)\n",
    "        mask = np.in1d(np.arange(y.size),subset_indices)\n",
    "        y=y[mask]\n",
    "        X=X[mask]\n",
    "        dates=dates[mask]\n",
    "        \n",
    "    ## Perform a train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                        shuffle=True, random_state=42)\n",
    "    ## train a random forest model\n",
    "    param_grid_rf = {\n",
    "    'rf__criterion': ['mse','mae'],\n",
    "    'rf__n_estimators': [10,15,20,25]\n",
    "    }\n",
    "\n",
    "    pipe_rf = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                              ('rf', RandomForestRegressor())])\n",
    "    \n",
    "    grid = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, iid=False, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    eval_rmse =  round(np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "    \n",
    "    ## retrain using all data\n",
    "    grid.fit(X, y)\n",
    "    model_name = re.sub(\"\\.\",\"_\",str(MODEL_VERSION))\n",
    "    if test:\n",
    "        saved_model = os.path.join(MODEL_DIR,\n",
    "                                   \"test-{}-{}.joblib\".format(tag,model_name))\n",
    "        print(\"... saving test version of model: {}\".format(saved_model))\n",
    "    else:\n",
    "        saved_model = os.path.join(MODEL_DIR,\n",
    "                                   \"sl-{}-{}.joblib\".format(tag,model_name))\n",
    "        print(\"... saving model: {}\".format(saved_model))\n",
    "        \n",
    "    joblib.dump(grid,saved_model)\n",
    "\n",
    "    m, s = divmod(time.time()-time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "\n",
    "    ## update log\n",
    "#    update_train_log(tag,(str(dates[0]),str(dates[-1])),{'rmse':eval_rmse},runtime,\n",
    "#                     MODEL_VERSION, MODEL_VERSION_NOTE,test=True)\n",
    "  \n",
    "\n",
    "def model_train(data_dir,test=False):\n",
    "    \"\"\"\n",
    "    funtion to train model given a df\n",
    "    \n",
    "    'mode' -  can be used to subset data essentially simulating a train\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "\n",
    "    if test:\n",
    "        print(\"... test flag on\")\n",
    "        print(\"...... subseting data\")\n",
    "        print(\"...... subseting countries\")\n",
    "        \n",
    "    ## fetch time-series formatted data\n",
    "    ts_data = fetch_ts(data_dir)\n",
    "\n",
    "    ## train a different model for each data sets\n",
    "    for country,df in ts_data.items():\n",
    "        \n",
    "        if test and country not in ['all','united_kingdom']:\n",
    "            continue\n",
    "        \n",
    "        _model_train(df,country,test=test)\n",
    "    \n",
    "def model_load(prefix='sl',data_dir=None,training=True):\n",
    "    \"\"\"\n",
    "    example funtion to load model\n",
    "    \n",
    "    The prefix allows the loading of different models\n",
    "    \"\"\"\n",
    "\n",
    "    if not data_dir:\n",
    "        #data_dir = os.path.join(\"..\",\"data\",\"cs-train\")\n",
    "        data_dir = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\")\n",
    "    \n",
    "#    models = [f for f in os.listdir(os.path.join(\".\",\"models\")) if re.search(\"sl\",f)]\n",
    "#    models = [f for f in os.listdir(os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\",\"models\")) if re.search(\"sl\",f)]\n",
    "    models = [f for f in os.listdir(os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\",\"models\"))]\n",
    "\n",
    "    if len(models) == 0:\n",
    "        raise Exception(\"Models with prefix '{}' cannot be found did you train?\".format(prefix))\n",
    "\n",
    "    all_models = {}\n",
    "    for model in models:\n",
    "        all_models[re.split(\"-\",model)[1]] = joblib.load(os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\",\"models\",model))\n",
    "\n",
    "    ## load data\n",
    "    ts_data = fetch_ts(data_dir)\n",
    "    all_data = {}\n",
    "    for country, df in ts_data.items():\n",
    "        X,y,dates = engineer_features(df,training=training)\n",
    "        dates = np.array([str(d) for d in dates])\n",
    "        all_data[country] = {\"X\":X,\"y\":y,\"dates\": dates}\n",
    "        \n",
    "    return(all_data, all_models)\n",
    "\n",
    "def model_predict(country,year,month,day,all_models=None,test=False):\n",
    "    \"\"\"\n",
    "    example funtion to predict from model\n",
    "    \"\"\"\n",
    "\n",
    "    ## start timer for runtime\n",
    "    time_start = time.time()\n",
    "\n",
    "    ## load model if needed\n",
    "    if not all_models:\n",
    "        all_data,all_models = model_load(training=False)\n",
    "    \n",
    "    ## input checks\n",
    "    if country not in all_models.keys():\n",
    "        raise Exception(\"ERROR (model_predict) - model for country '{}' could not be found\".format(country))\n",
    "\n",
    "    for d in [year,month,day]:\n",
    "        if re.search(\"\\D\",d):\n",
    "            raise Exception(\"ERROR (model_predict) - invalid year, month or day\")\n",
    "    \n",
    "    ## load data\n",
    "    model = all_models[country]\n",
    "    data = all_data[country]\n",
    "\n",
    "    ## check date\n",
    "    target_date = \"{}-{}-{}\".format(year,str(month).zfill(2),str(day).zfill(2))\n",
    "    print(target_date)\n",
    "\n",
    "    if target_date not in data['dates']:\n",
    "        raise Exception(\"ERROR (model_predict) - date {} not in range {}-{}\".format(target_date,\n",
    "                                                                                    data['dates'][0],\n",
    "                                                                                    data['dates'][-1]))\n",
    "    date_indx = np.where(data['dates'] == target_date)[0][0]\n",
    "    query = data['X'].iloc[[date_indx]]\n",
    "    \n",
    "    ## sainty check\n",
    "    if data['dates'].shape[0] != data['X'].shape[0]:\n",
    "        raise Exception(\"ERROR (model_predict) - dimensions mismatch\")\n",
    "\n",
    "    ## make prediction and gather data for log entry\n",
    "    y_pred = model.predict(query)\n",
    "    y_proba = None\n",
    "    if 'predict_proba' in dir(model) and 'probability' in dir(model):\n",
    "        if model.probability == True:\n",
    "            y_proba = model.predict_proba(query)\n",
    "\n",
    "\n",
    "    m, s = divmod(time.time()-time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "\n",
    "    ## update predict log\n",
    "#    update_predict_log(country,y_pred,y_proba,target_date,\n",
    "#                       runtime, MODEL_VERSION, test=test)\n",
    "    \n",
    "    return({'y_pred':y_pred,'y_proba':y_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"\n",
    "    basic test procedure for model.py\n",
    "    \"\"\"\n",
    "\n",
    "    ## train the model\n",
    "    print(\"TRAINING MODELS\")\n",
    "    data_dir = os.path.join(\"c:\\\\\",\"data_ml\",\"cs-train\")\n",
    "    model_train(data_dir,test=False)\n",
    "\n",
    "    ## load the model\n",
    "    print(\"LOADING MODELS\")\n",
    "    all_data, all_models = model_load()\n",
    "    print(\"... models loaded: \",\",\".join(all_models.keys()))\n",
    "\n",
    "    ## test predict\n",
    "    country='all'\n",
    "    year='2018'\n",
    "    month='01'\n",
    "    day='05'\n",
    "    result = model_predict(country,year,month,day)\n",
    "    print(result)\n",
    "\n",
    "TRAINING MODELS\n",
    "... loading ts data from files\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-all-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-eire-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-france-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-germany-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-hong_kong-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-netherlands-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-norway-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-portugal-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-singapore-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-spain-0_1.joblib\n",
    "\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
    "  warnings.warn(\n",
    "\n",
    "... saving model: c:\\data_ml\\cs-train\\models\\sl-united_kingdom-0_1.joblib\n",
    "LOADING MODELS\n",
    "... loading ts data from files\n",
    "... models loaded:  all,eire,france,germany,hong_kong,netherlands,norway,portugal,singapore,spain,united_kingdom\n",
    "... loading ts data from files\n",
    "2018-01-05\n",
    "{'y_pred': array([181009.5099]), 'y_proba': None}\n",
    "\n",
    "Case study part 3\n",
    "In [ ]:\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "\n",
    "if not os.path.exists(\"models\") :\n",
    "    os.mkdir(\"models\")\n",
    "#MODEL_DIR = \"models\"\n",
    "#DATA_DIR = \"data\"\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "## preprocessing pipeline\n",
    "numeric_features = ['age', 'num_streams']\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['country', 'subscriber_type']\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                               ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "def load_aavail_data():\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, r\"aavail-target.csv\"))\n",
    "\n",
    "    ## pull out the target and remove uneeded columns\n",
    "    _y = df.pop('is_subscriber')\n",
    "    y = np.zeros(_y.size)\n",
    "    y[_y==0] = 1 \n",
    "    df.drop(columns=['customer_id','customer_name'],inplace=True)\n",
    "    return(df,y)\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "## load data (you may need to adjust the location of the data to match your system)\n",
    "X,y = load_aavail_data()\n",
    "\n",
    "## train test split check model performance (assumes you have already grid-searched to tune model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "params = {'n_estimators': 100,'max_depth':2}   \n",
    "clf = ensemble.RandomForestClassifier(**params)\n",
    "pipe = Pipeline(steps=[('pre', preprocessor),\n",
    "                       ('clf',clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "## retrain using all of the data\n",
    "pipe.fit(X, y)\n",
    "saved_model = 'aavail-rf.joblib'\n",
    "joblib.dump(pipe, os.path.join(MODEL_DIR, saved_model))\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "%%writefile app.py\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "import joblib\n",
    "import socket\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    html = \"<h3>Hello {name}!</h3>\" \\\n",
    "           \"<b>Hostname:</b> {hostname}<br/>\"\n",
    "    return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname())\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    \n",
    "    ## input checking\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (predict): did not receive request data\")\n",
    "        return jsonify([])\n",
    "\n",
    "    query = request.json\n",
    "    query = pd.DataFrame(query)\n",
    "    \n",
    "    if len(query.shape) == 1:\n",
    "         query = query.reshape(1, -1)\n",
    "\n",
    "    y_pred = model.predict(query)\n",
    "    \n",
    "    return(jsonify(y_pred.tolist()))        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    saved_model = 'aavail-rf.joblib'\n",
    "    model = joblib.load(os.path.join(MODEL_DIR, saved_model))\n",
    "    app.run(host='0.0.0.0', port=8080,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "cython\n",
    "numpy\n",
    "flask\n",
    "pandas\n",
    "scikit-learn\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "%%writefile Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.7.5-stretch\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "python3-dev \\\n",
    "build-essential    \n",
    "        \n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "ADD . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "## create some new data\n",
    "X_new_data = {}\n",
    "X_new_data['country'] = ['united_states','united_states','singapore','united_states','singapore']\n",
    "X_new_data['age'] = [28,30,33,24,39]\n",
    "X_new_data['subscriber_type'] = ['aavail_premium','aavail_basic','aavail_basic','aavail_basic','aavail_unlimited']\n",
    "X_new_data['num_streams'] = [9,19,14,33,20]\n",
    "X_new = pd.DataFrame(X_new_data)\n",
    "X_new.head()\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "## data needs to be in dict format for JSON\n",
    "query = X_new.to_dict()\n",
    "\n",
    "## test the Flask API\n",
    "# port = 8080\n",
    "# r = requests.post('http://0.0.0.0:{}/predict'.format(port),json=query)\n",
    "\n",
    "## test the Docker API\n",
    "port = 4000\n",
    "r = requests.post('http://0.0.0.0:{}/predict'.format(port),json=query)\n",
    "\n",
    "response = literal_eval(r.text)\n",
    "print(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
